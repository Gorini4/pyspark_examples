{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb3dfa-dcbb-489b-9250-8a003917314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba5aba-f394-4166-8263-8708c18b1b2e",
   "metadata": {},
   "source": [
    "# Apache Arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bac197-5e22-46de-a95b-a175bb206c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import pyarrow\n",
    "\n",
    "df = pandas.DataFrame({'one': [-1, np.nan, 2.5],\n",
    "                   'two': ['foo', 'bar', 'baz'],\n",
    "                   'three': [True, False, True]},\n",
    "                   index=list('abc'))\n",
    "\n",
    "\n",
    "table = pyarrow.Table.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b5dfb-cfb0-4c4c-b328-5dd8b6fe9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import parquet\n",
    "\n",
    "parquet.write_table(table, 'example.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f2df7-c720-403e-9641-2fa80545a3e1",
   "metadata": {},
   "source": [
    "# Обычные UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ccd4be9-d59e-40e2-8457-c3d1dd16545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n",
      "| id|    name|age|\n",
      "+---+--------+---+\n",
      "|  1|John Doe| 21|\n",
      "+---+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3df116a-1f9a-422e-aeb2-b01288d46cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- string_length: string (nullable = true)\n",
      " |-- string_length2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string_length = udf(lambda s: len(s))\n",
    "\n",
    "@udf(\"int\")\n",
    "def string_length2(s):\n",
    "    return len(s)\n",
    "\n",
    "df.select(string_length(col('name')).alias('string_length'), string_length2(col('name')).alias('string_length2')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9de630-c5b9-4282-be21-a2f7164766f7",
   "metadata": {},
   "source": [
    "# Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e40a49cb-912b-4a97-9294-86dca732287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(StringType(), PandasUDFType.SCALAR)\n",
    "def to_upper(s):\n",
    "    return s.str.upper()\n",
    "\n",
    "@pandas_udf(IntegerType(), PandasUDFType.SCALAR)\n",
    "def add_one(x):\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "096f50c8-ef00-45de-b7fa-fd7bdb70c4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|to_upper(name)|add_one(age)|\n",
      "+--------------+------------+\n",
      "|      JOHN DOE|          22|\n",
      "+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_upper(\"name\"), add_one(\"age\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d99f71ad-1721-4311-9d4f-44f539697839",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99649ea8-c338-4652-bcf5-61b3dd4dde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"id long, value double\", PandasUDFType.GROUPED_MAP)\n",
    "def normalize(pdf):\n",
    "    v = pdf.value\n",
    "    return pdf.assign(value=(v - v.mean()) / v.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7a781a9-56f7-4e3a-ad8f-c3f276fbc5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/group_ops.py:103: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|              value|\n",
      "+---+-------------------+\n",
      "|  1|-0.7071067811865475|\n",
      "|  1| 0.7071067811865475|\n",
      "|  2|-0.8320502943378437|\n",
      "|  2|-0.2773500981126146|\n",
      "|  2| 1.1094003924504583|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"id\").apply(normalize).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fb453b7-b74e-4b10-bf97-8adaa9760a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
    "def mean_udf(v):\n",
    "    return v.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "818a1396-b7cd-40f4-bbaa-c7ab79789c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id|value|mean_v|\n",
      "+---+-----+------+\n",
      "|  1|  1.0|   1.5|\n",
      "|  1|  2.0|   1.5|\n",
      "|  2|  3.0|   6.0|\n",
      "|  2|  5.0|   6.0|\n",
      "|  2| 10.0|   6.0|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window \\\n",
    "    .partitionBy('id') \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "df.withColumn('mean_v', mean_udf(df['value']).over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d61023-8ac1-4d53-b5a1-871450b3b690",
   "metadata": {},
   "source": [
    "# Spark 3 UDF (Type Hints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "245c41e5-2868-40a8-b298-9ad5a3f16016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d229195d-fb7f-459d-ba8e-f5f679f5f5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|pandas_plus_one(id)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  2|\n",
      "|                  3|\n",
      "|                  4|\n",
      "|                  5|\n",
      "|                  6|\n",
      "|                  7|\n",
      "|                  8|\n",
      "|                  9|\n",
      "|                 10|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('long', PandasUDFType.SCALAR)\n",
    "def pandas_plus_one(v):\n",
    "    return v + 1\n",
    "\n",
    "spark.range(10).select(pandas_plus_one(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a98c35a2-cb7d-487d-bf8c-5aff20f03d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|pandas_plus_one(id)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  2|\n",
      "|                  3|\n",
      "|                  4|\n",
      "|                  5|\n",
      "|                  6|\n",
      "|                  7|\n",
      "|                  8|\n",
      "|                  9|\n",
      "|                 10|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('long')\n",
    "def pandas_plus_one(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "\n",
    "spark.range(10).select(pandas_plus_one(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c418c0aa-4567-4f26-a121-3d8850bcd6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|multiply_two(id, id)|\n",
      "+--------------------+\n",
      "|                   0|\n",
      "|                   1|\n",
      "|                   4|\n",
      "|                   9|\n",
      "|                  16|\n",
      "|                  25|\n",
      "|                  36|\n",
      "|                  49|\n",
      "|                  64|\n",
      "|                  81|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('long', PandasUDFType.SCALAR_ITER) # New in 3.0\n",
    "def multiply_two(iterator):\n",
    "    return (a * b for a, b in iterator)\n",
    "\n",
    "spark.range(10).select(multiply_two(\"id\", \"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d48cf-46a0-4fdb-85e7-5ccc19f4e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Tuple       \n",
    "\n",
    "@pandas_udf(\"long\")\n",
    "def multiply_two(iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    return (a * b for a, b in iterator)\n",
    "\n",
    "spark.range(10).select(multiply_two(\"id\", \"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1682c638-f3e7-4698-aa3f-ffdd4e444d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+\n",
      "|time| id| v1| v2|\n",
      "+----+---+---+---+\n",
      "|1201|  1|1.0|  x|\n",
      "|1202|  1|3.0|  x|\n",
      "|1201|  2|2.0|  y|\n",
      "|1202|  2|4.0|  y|\n",
      "+----+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    [(1201, 1, 1.0), (1201, 2, 2.0), (1202, 1, 3.0), (1202, 2, 4.0)],\n",
    "    (\"time\", \"id\", \"v1\"))\n",
    "df2 = spark.createDataFrame(\n",
    "    [(1201, 1, \"x\"), (1201, 2, \"y\")], (\"time\", \"id\", \"v2\"))\n",
    "\n",
    "def asof_join(left: pd.DataFrame, right: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.merge_asof(left, right, on=\"time\", by=\"id\")\n",
    "\n",
    "df1.groupby(\"id\").cogroup(\n",
    "    df2.groupby(\"id\")\n",
    ").applyInPandas(asof_join, \"time int, id int, v1 double, v2 string\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "508ad3c0-fc23-4902-9875-ddf43a30ea6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "|time| id| v1|\n",
      "+----+---+---+\n",
      "|1201|  1|1.0|\n",
      "|1201|  2|2.0|\n",
      "|1202|  1|3.0|\n",
      "|1202|  2|4.0|\n",
      "+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e74f8d85-3376-4251-81f0-5bc866076cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "|time| id| v2|\n",
      "+----+---+---+\n",
      "|1201|  1|  x|\n",
      "|1201|  2|  y|\n",
      "+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18634b80-1a27-4878-90fc-5b8ae6fab8fc",
   "metadata": {},
   "source": [
    "# Scala UDF in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "716e7455-e6ce-4841-bd37-8398f4eaa4cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_ipToIntUDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Column(_ipToIntUDF\u001b[38;5;241m.\u001b[39mapply(_to_seq(sc, [ipString], _to_java_column)))\n\u001b[1;32m     15\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m192.168.0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m192.168.0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtoDF(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m df\\\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mip_int_scala\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43m_ipToIntUDF\u001b[49m(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mip\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\\\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name '_ipToIntUDF' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.column import Column\n",
    "from pyspark.sql.column import _to_java_column\n",
    "from pyspark.sql.column import _to_seq\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "def udfIpToIntScalaWrapper(ipString):\n",
    "    _ipToIntUDF = sc._jvm.CustomUDFs.ipToIntUDF()\n",
    "    return Column(_ipToIntUDF.apply(_to_seq(sc, [ipString], _to_java_column)))\n",
    "\n",
    "df = spark.createDataFrame([\"192.168.0.1\", \"192.168.0.2\"], \"string\").toDF(\"ip\")\n",
    "\n",
    "df\\\n",
    "    .withColumn(\"ip_int_scala\", udfIpToIntScalaWrapper(col(\"ip\")))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0837f0a-45d7-4622-a6d0-01143df2bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"ips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0a2a5a9-47aa-40d6-9f49-d697bf61c59e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_ROUTINE] Cannot resolve function `ipToIntUDF` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`].; line 1 pos 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect ip, ipToIntUDF(ip) from ips\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_ROUTINE] Cannot resolve function `ipToIntUDF` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`].; line 1 pos 11"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select ip, ipToIntUDF(ip) from ips\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4013f42-e3c7-4965-9b3f-aecce7e3ad26",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "var is none of Java Object, Java Class or Java Member",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mudf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mipToIntUDF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCustomUDFs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mipToIntUDF\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py:600\u001b[0m, in \u001b[0;36mUDFRegistration.register\u001b[0;34m(self, name, f, returnType)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m returnType \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    599\u001b[0m         returnType \u001b[38;5;241m=\u001b[39m StringType()\n\u001b[0;32m--> 600\u001b[0m     return_udf \u001b[38;5;241m=\u001b[39m \u001b[43m_create_udf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturnType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPythonEvalType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSQL_BATCHED_UDF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m     register_udf \u001b[38;5;241m=\u001b[39m return_udf\u001b[38;5;241m.\u001b[39m_unwrapped  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mudf()\u001b[38;5;241m.\u001b[39mregisterPython(name, register_udf\u001b[38;5;241m.\u001b[39m_judf)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py:82\u001b[0m, in \u001b[0;36m_create_udf\u001b[0;34m(f, returnType, evalType, name, deterministic)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_udf\u001b[39m(\n\u001b[1;32m     72\u001b[0m     f: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any],\n\u001b[1;32m     73\u001b[0m     returnType: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataTypeOrString\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUserDefinedFunctionLike\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# Set the name of the UserDefinedFunction object to be the name of function f\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     udf_obj \u001b[38;5;241m=\u001b[39m UserDefinedFunction(\n\u001b[1;32m     80\u001b[0m         f, returnType\u001b[38;5;241m=\u001b[39mreturnType, name\u001b[38;5;241m=\u001b[39mname, evalType\u001b[38;5;241m=\u001b[39mevalType, deterministic\u001b[38;5;241m=\u001b[39mdeterministic\n\u001b[1;32m     81\u001b[0m     )\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mudf_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py:420\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# It is possible for a callable instance without __name__ attribute or/and\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# __module__ attribute to be wrapped here. For example, functools.partial. In this case,\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# we should avoid wrapping the attributes from the wrapped function to the wrapper\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# function. So, we take out these attribute names from the default names to set and\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# then manually assign it after being wrapped.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m assignments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    416\u001b[0m     a \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m functools\u001b[38;5;241m.\u001b[39mWRAPPER_ASSIGNMENTS \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m a \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__module__\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m )\n\u001b[1;32m    419\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@functools\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwraps\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massigned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massignments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 420\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mwrapper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mColumnOrName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mColumn\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m wrapper\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/functools.py:52\u001b[0m, in \u001b[0;36mupdate_wrapper\u001b[0;34m(wrapper, wrapped, assigned, updated)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m assigned:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1253\u001b[0m, in \u001b[0;36mJavaMember.__doc__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__doc__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;66;03m# The __doc__ string is used by IPython/PyDev/etc to generate\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;66;03m# help string, therefore provide useful help\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1253\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_doc \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_help\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_doc\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:593\u001b[0m, in \u001b[0;36mgateway_help\u001b[0;34m(gateway_client, var, pattern, short_name, display)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gateway_help(\n\u001b[1;32m    590\u001b[0m         gateway_client, var, pattern, short_name\u001b[38;5;241m=\u001b[39mshort_name,\n\u001b[1;32m    591\u001b[0m         display\u001b[38;5;241m=\u001b[39mdisplay)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    594\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvar is none of Java Object, Java Class or Java Member\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    596\u001b[0m help_page \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (display):\n",
      "\u001b[0;31mPy4JError\u001b[0m: var is none of Java Object, Java Class or Java Member"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"ipToIntUDF\", sc._jvm.CustomUDFs.ipToIntUDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db13541d-e298-44bb-805f-2ab00b32a9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
